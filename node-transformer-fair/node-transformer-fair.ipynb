{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tale of the NODE-Transformer: an unsuccessful, deeply non-ecological yet relatively fruitful study about cross-breeding Transformer with Neural-ODE\n",
    "\n",
    "Code is available in Apache2 License on [Github Project](https://github.com/mandubian/pytorch-neural-ode/)\n",
    "\n",
    "You can contact me on Twitter [@mandubian](http://twitter.com/mandubian)\n",
    "\n",
    "> Disclaimer: I don't pretend to be an expert about ODE and optimizing mathematics so sorry if I'm not precise or even wrong on some aspects or intuitions... Yet, I'm continuously learning like my models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Once upon a time, there was a guy who decided to give life to a creature, half-Transformer, half-Neural-ODE. That seemed a cool idea at first sight but it revealed to be a monster, not so cute, always hungry to swallow all the single GPU power without beating any SOTA. Yet, along his misadventures, he met Prince Serendip and discovered many interesting knowledge.\n",
    "\n",
    "In this _tale of the NODE-Transformer_, you will know more about those weird creatures, the Transformer and the Neural-ODE and you will see how they were hybridized in a very foolish way. You will discover the narrow link between mathematical complexity and knowledge complexity and how it is naturally managed by the ODE-Solver in NODE-Transformer. You will then see how NODE-Transformer encoder seems to learn knowledge keeping a stable complexity while decoder builds more and more complex knowledge to fulfill the output task. Yet, NODE-Transformer appears to be much less performant than a classic Transformer, even with 1-layer of encoder/decoder. Searching for possible causes, you will learn that Neural-ODE can't represent all kind of functions but can be augmented to overcome those limitations. Then, despite augmented ODE, despite reduced scope to decoder only and finally a few tricks of optimization and regularization, NODE-Transformer never reaches classic Transformer's performance.\n",
    "\n",
    "Finally, despite this creature might not be as nice as he expected, you will understand why the guy above was still liking his ugly creature and convinced it was worth the efforts.\n",
    "\n",
    "Now, let me tell you of the days of not so high adventures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer as residual neural network\n",
    "\n",
    "First a quick reminder on the architecture of [Transformer model](http://arxiv.org/abs/1706.03762) is well known:\n",
    "\n",
    "<img src=\"../media/Transformer.svg\" width=\"50%\"/>\n",
    "\n",
    "Transformer is built on an encoder and a decoder which are both made of `N` successive self-attention and feed-forward layers with residual connections.\n",
    "So, beyond their a-priori complexity, Transformer encoder and decoder can be seen as residual neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODE as continuous limit of residual network\n",
    "\n",
    "As you may known, a residual neural layer is defined by the following equation:\n",
    "\n",
    "<img src=\"../media/residual_network.png\" width=\"15%\"/>\n",
    "\n",
    "Residual connection uses the input data and just adds an update to it. It allows not to forget completely the original data while learning to update it progressively along the layers.\n",
    "\n",
    "In paper [Neural Ordinary Differential Equations](http://arxiv.org/abs/1806.07366), Neural ODE is defined as the continuous limit of a residual neural network:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{t+1} - h_{t} = f(h_{t}, {\\theta}).1 \\\\\n",
    "h_{t+\\delta_{t}} - h_{t} = f(h_{t}, {\\theta}).\\delta_{t} \\\\\n",
    "{\\frac {h_{t+\\delta_{t}} - h_{t}}{\\delta_{t}}} = f(h_{t}, {\\theta}) \\\\\n",
    "\\lim_{\\delta_{t}\\to0} {\\frac {h_{t+\\delta_{t}} - h_{t}}{\\delta_{t}}} = f(h_{t}, {\\theta}) \\\\\n",
    "{\\frac {dh(t)}{dt}} = f(h(t), t, {\\theta})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This definition is a well-known Ordinary Differential Equation which are classicaly solved with ODE Solvers (Euler, Runge-Kutta variants as shown in next figure).\n",
    "\n",
    "<img src=\"../media/Runge-Kutta_slopes.svg\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "In the [Neural ODE paper](http://arxiv.org/abs/1806.07366), without entering in details, such equation is proven to to be learnable by a neural network trained by back-propagation on an augmented version of gradient and a call to an ODE Solver.\n",
    "\n",
    "To give some intuition, in a 5-layers residual network sharing weights on all layers (see following figure), backpropagation is performed on 5 fixed steps `[0.0, 0.25, 0.5, 0.75, 1.0]` in interval `[0.0, 1.0]` from the output layer back to the input layer.\n",
    "\n",
    "Meanwhile, in a 1-layer Neural ODE, backpropagation is performed in a shallow way by an ODE Solver which explores the interval `[0.0, 1.0]` in a continuous and dynamic way according to its solving algorithm. Modern ODE Solvers (like the DOPRI5 aka Dormandâ€“Prince method used in current study) use adaptive steps dynamically refined based on max error approximation until they reach enough precision.\n",
    "\n",
    "<img src=\"../media/node_grad.png\" width=\"50%\"/>\n",
    "\n",
    "So, by relying on ODE Solver's continuous and dynamic sampling of the steps of backpropagation, a Neural ODE network can be seen as a network able to adapt its own depth during training depending on the loss horizon it explores. This feature is very appealing as it means you can have a network that is able to adapt its own complexity to the task it's trying to learn without increasing the number of parameters in the network.\n",
    "\n",
    "In paper [Neural ODEs as the deep limit of resnets with constant weights](https://arxiv.org/pdf/1906.12183v1.pdf), authors prove mathematically that Neural ODE are the limit of resnets when depth grows infinitely (with a few assumptions and constraints).\n",
    "\n",
    "Beyond the mathematical aspects of Neural ODE, those networks have proven to be very efficient at modelising continuous flows and continuous time series, better than RNN and with fewer parameters.\n",
    "\n",
    "<img src=\"../media/node_spirals.png\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The stupid idea: a Neural-ODE Transformer\n",
    "\n",
    "Having been playing with Neural ODE, having used intensively transformers in NLP domain, suddenly a very stupid idea materialized:\n",
    "\n",
    "> **What if Neural ODE layers replace residual layers in Transformers?**\n",
    "\n",
    "When one has such foolish idea, one has to try to un-think as much as possible and be crazy till the end because this is in such ideas that one can find the best or the worst things. But in both cases, one will have fun and learn a lot of things!\n",
    "\n",
    "So it was decided to hack [Transformer from cool Facebook FairSeq library](https://fairseq.readthedocs.io/en/latest/#) and [Pytorch Neural ODE TorchDiffEq](https://github.com/rtqichen/torchdiffeq) and build **the creature half-Transformer, half-Neural-ODE: the NODE-Transformer**\n",
    "\n",
    "The code is not polished and complete yet but it is available on [this github](https://github.com/mandubian/pytorch-neural-ode/tree/master/node-transformer-fair/node_transformer) based on Fairseq and TorchDiffEq.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODE-Transformer Architecture\n",
    "\n",
    "Here is the first **NODE-Transformer architecture**:\n",
    "\n",
    "<img src=\"../media/Node-Transformer-Full.svg\" width=\"50%\"/>\n",
    "\n",
    "You can see that it still has separated Encoder and Decoder pipelines. But, now instead of several layers of self-attention/feed-forward with residual connections, it has one single layer of Neural-ODE solving a single self-attention/feed-forward sub-network without any residual connection.\n",
    "\n",
    "After some hacking on FairSeq and Torchdiffeq, it was time to test it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Transformer Training on multi30k-en-ge\n",
    "\n",
    "First of all, to have a reference, default transformer, as defined in Fairseq, was trained on Multi30k-EN-GE translation task.\n",
    "\n",
    "Here is the best loss plot of 2 training sessions:\n",
    "\n",
    "- default Transformer with 6-layers encoder/decoder and all default parameters from FairSeq (<span style=\"color:blue\">blue</span>)\n",
    "- a classic Transformer with 1-layer encoder/decoder and all default parameters from FairSeq (<span style=\"color:green\">green</span>)\n",
    "\n",
    "<img src=\"../media/transformer_full_decoder_1layer_best_loss.svg\" width=\"50%\"/>\n",
    "\n",
    "6-layers transformer converges and reaches a lower loss (3.04) but not very far from 1-layer (3.194).\n",
    "\n",
    "1-layer encoder/decoder absorbs most of Multi30k task complexity and 6 times more parameters just gives a light improvement.\n",
    "\n",
    "\n",
    "_Naturally, for a translation task, other metrics like BLEU or perplexity are provided in Fairseq but we'll focus here on loss as other metrics behave the same in all tests performed in this study._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NODE Transformer Training on multi30k-en-ge\n",
    "\n",
    "Now, NODE-transformer as described in architecture above was trained on the same task as classic Transformers.\n",
    "\n",
    "Here is the plot of a training session (<span style=\"color:grey\">train loss</span>/<span style=\"color:orange\">validation loss</span>):\n",
    "\n",
    "<img src=\"../media/transformer_1layer_node_transformer_full_loss.svg\" width=\"50%\"/>\n",
    "\n",
    "Good point, it looks like converging: for validation dataset, it reaches a minimum before rising again. For training dataset, it goes down and tends to overfit on training data as a normal Transformer.\n",
    "\n",
    "----\n",
    "\n",
    "**Conclusion 0: NODE-Transformer learns something**\n",
    "\n",
    "----\n",
    "\n",
    "Next is a plot for the compared best loss between:\n",
    "\n",
    "- a classic Transformer with 1-layer encoder/decoder in <span style=\"color:green\">green</span>\n",
    "- a NODE Transformer with NODE encoder/decoder in <span style=\"color:orange\">orange</span>\n",
    "\n",
    "<img src=\"../media/transformer_1layer_node_transformer_full.svg\" width=\"50%\"/>\n",
    "\n",
    "We can see that NODE Transformer converges down to a limit loss of ~4.5, much higher than a classic 1-layer transformer (~3.2).\n",
    "\n",
    "----\n",
    "\n",
    "**Conclusion 1: As is, NODE transformer isn't able to reach the performance of a normal transformer with 1 single layer**\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's see the same chart in relative time (number of hours in absciss):\n",
    "\n",
    "<img src=\"../media/transformer_1layer_node_transformer_full_relative.svg\" width=\"50%\"/>\n",
    "\n",
    "Training this NODE transformer took more than 1.5 day (35h) against 1.5 hour for classic transformer. That's also why it was early-stopped after stagnating a bit because this experiment is clearly not very ecological after all ;)\n",
    "\n",
    "----\n",
    "\n",
    "**Conclusion 2: Training a NODE transformer is much slower and sub-efficient than classic transformer.**\n",
    "\n",
    "----\n",
    "\n",
    "> Those disappointing results lead to wonder why it clearly learns less well than a 1-layer transformer. First idea was naturally to check ODE Solver parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ODE-Solver Absolute and relative max error tolerance\n",
    "\n",
    "The first point that arises when using an ODE Solver such as DOPRI5 (Dormandâ€“Prince method) are the 2 main hyper-parameters:\n",
    "\n",
    "- absolute error tolerance (ATOL)\n",
    "- relative error tolerance (RTOL)\n",
    "\n",
    "Without going too deep in mathematics, let say that in ODE solver deriving from Runge-Kutta method, an approximation of the max error between ground-truth value and estimated value can be computed. So, it's possible to fix a max error tolerance that allows the ODE Solver to know whether the estimated value is a viable solution. This description of both parameters gives some more intuition: https://www.mathworks.com/help/matlab/math/troubleshoot-common-ode-problems.html#bu8pzr7\n",
    "\n",
    "DOPRI5 is an adaptive explicit ODE Solver. It means that, if it can't reach a low-enough error approximation, the solver can _refine the sampling steps_ until it finds an acceptable value. The more it refines the steps, the more the ODE Solver calls the neural model function until it finds an acceptable solution depending on the error tolerance.\n",
    "\n",
    "In previous training session, ATOL/RTOL were fixed to 0.001 meaning \"roughly\" that it can't be more precise than 3 digits. It took 1.5 day to train with this error tolerance on a 1080-TI GPU. Lower ATOL/RTOL would have been a good test but it's not decent with current computing power (specially this summer in France where it has been >40Â°C). That's why it was decided to increase ATOL/RTOL to 0.01 and check what happens.\n",
    "\n",
    "Here is the Best Loss plot in relative time of a new training session with ATOL/RTOL set to 0.01:\n",
    "\n",
    "<img src=\"../media/transformer_full_lower_atol.svg\" width=\"50%\"/>\n",
    "\n",
    "It converges down to a loss limit of ~5.98, much higher than the previous value ~4.5 with ATOL/RTOL of 0.001. So, it doesn't learn as well as using a lower error tolerance which was expected. Yet, it's interesting to remark that it reached this local minimum in a bit more than 4h compared to the previous 35h. So, it demonstrates that a bigger error tolerance truly leads to faster ODE Solving.\n",
    "\n",
    "Now, let's observe the number of calls made by the ODE Solver to the neural networks in both encoder and decoder during the training:\n",
    "\n",
    "#### NODE-Transformer Decoder ODE Calls\n",
    "\n",
    "<img src=\"../media/transformer_full_lower_atol_nfe_decoder.svg\" width=\"50%\"/>\n",
    "\n",
    "We see here that the number of calls increases progressively with high peak values and becomes noisier when the NODE-Transformer saturates and doesn't optimize anymore.\n",
    "\n",
    "This progressive increase of calls could mean the ODE Solver is exploring a relatively \"simple\" loss landscape at beginning of training. We can imagine knowledge that is learnt at first is basic in general. We can make an analogy with a human-crafted NLP pipeline. At first, you do basic structural tasks: tag words, lemmatize tags, find dependencies between lemma etc...\n",
    "\n",
    "Then, the ODE Solver is encountering more complex loss landscape and needs to sample its surroundings finer and finer to evaluate viable values. This corresponds to the increasing complexity of knowledge that is learnt during training: from basic language forms, it needs to model syntax and semantics, contextualize and build relationships between entities.\n",
    "\n",
    "So, it's really interesting to validate this intuition that a Neural-ODE is truly increasing its own complexity dynamically to accomodate to the increasing knowledge complexity along training. But it's not just like a Resnet increasing arbitrarily its depth, the ODE-solver is naturally refining its computations depending on the complexity of the loss landscape it's exploring.\n",
    "\n",
    "----\n",
    "\n",
    "**A bit of philosophy: this idea is mind-blowing as the optimization mechanism is effectivey forcing the ODE-Solver to increase its own complexity to solve more complex equations and at same time, optimizing requires to model more and more complex knowledge to learn the task. is it mathematical complexity driving knowledge complexity or knowledge complexity driving mathematical complexity? More like chicken and egg maybe ;)**\n",
    "\n",
    "---- \n",
    "It also proves we still need to work a lot to understand and study optimizers more and more. Let's remark that no paper proved anything about those aspects, theoretically speaking so it's still experimental hypothesis for now.\n",
    "\n",
    "The peak values are interesting too. It means there are batches which contain much more complicated information to represent and the ODE-Solver has hard time to estimate a value for that. Those very peaky values are also an issue for Neural-ODE trainings as one single batch can end in a very long duration of computation or even end in never-ending computations. This is an issue in current Neural-ODE as some trainings might never end if it can't solve a single batch as it never reaches a viable estimation. Maybe we could consider skipping those too complex batches and re-focus on them later when ODE-Solver can digest a higher complexity. To be studied in next version...\n",
    "\n",
    "Finally, the noisier aspect at the end of training is quite logical as knowledge then is really at its maximum of complexity and might vary a lot depending on the batches.\n",
    "\n",
    "\n",
    "#### NODE-Transformer Encoder ODE Calls\n",
    "\n",
    "<img src=\"../media/transformer_full_lower_atol_nfe_encoder.svg\" width=\"50%\"/>\n",
    "\n",
    "This is really interesting: the encoder here learns almost in constant number of calls, except a few peak values (corresponding to batches of validation datasets between epochs).\n",
    "\n",
    "Remember that the NODE-Transformer decoder above increased the number of calls during training and the intuition is that it is building more and more complex knowledge.\n",
    "\n",
    "It would require an in-depth study in itself but we can suggest that encoding task might be a quite \"brute-force\" and systematic mechanism, a bit like extracting and storing all information about data in a database: the items, the lemma, even the basic relations like ordering or context. But it's raw information, it doesn't really contain complex knowledge and it doesn't really care about the final task to learn. Meanwhile, the decoder knows the final task to learn and has the responsibility to use that encoded raw information and model the fine relations between entities. Thus, it naturally models more and more complex knowledge.\n",
    "\n",
    "----\n",
    "**Conclusion 3: Neural ODE really increase its complexity during training when it needs to model more complex knowledges.**\n",
    "\n",
    "----\n",
    "\n",
    "**Conclusion 4: NODE-Transformer Decoder truly increases its complexity during training while Encoder keeps it almost constant. It might suggest complex knowledge modelization linked to the task happens more on the decoding part of the network. Deeper study on how knowledge is built in a transformer would be required to validate this intuition.**\n",
    "\n",
    "----\n",
    "\n",
    "The same study with lower ATOL/RTOL like 0.0001 and 0.00001 and longer trainings would be good to see whether it can learn more and reach classic transformer performances. But it requires having more powerful computing resources than a single 1080-TI GPU.\n",
    "\n",
    "----\n",
    "\n",
    "**REQUEST FOR RESOURCES: If you like this topic and have GPU resources that you can share for free and want to help perform more studies on that idea, don't hesitate to contact me on Twitter @mandubian or Github, I'd be happy to consume your resources ;)**\n",
    "\n",
    "----\n",
    "\n",
    "> We have seen here the NODE-Transformer learns something, builds some complex knowledge but doesn't reach Transformer performances. So, we can wonder whether Neural-ODE based on self-attention from transformer is even able to represent translation task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can Neural-ODE learn a continuous function that represents translation task based on self-attention + feed-forward ?\n",
    "\n",
    "Let's be honest, the narrator has no theoretical answer to this question as he hasn't studied ODE math enough to have a proven fact on those points.\n",
    "\n",
    "Yet, he has read this very interesting paper [Augmented Neural ODEs](http://arxiv.org/abs/1904.01681).\n",
    "\n",
    "### Introduction to Augmented Neural ODE\n",
    "\n",
    "> **Please note that next figures are just copied from the [paper](http://arxiv.org/abs/1904.01681)**\n",
    "\n",
    "The authors in this paper demonstrate that Neural-ODE cannot represent all types of functions and fail at this kind of functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(âˆ’1) = 1 \\\\\n",
    "h(1) = âˆ’1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<img src=\"../media/func_aug.svg\" width=\"30%\"/>\n",
    "\n",
    "ODE Solver is made to solve continuous flows, not this kind of functions with discrete values or discontinuities in feature space. When solving equations, it's trying to \"draw\" continuous trajectories (aka ODE flow) from one point to the other in the feature space in the following way:\n",
    "\n",
    "<img src=\"../media/func_traj.png\" width=\"30%\"/>\n",
    "\n",
    "But in the paper, they prove that if ODE flow has 2 trajectories of this kind with different initialization conditions and that intersects, it ends in a contradiction as it would mean initialization conditions are the same. So it's impossible for an ODE flow to \"draw\" crossing trajectories.\n",
    "\n",
    "For the above function, ODE flow can't cross trajectories so it ends in something like (full red and blue lines):\n",
    "\n",
    "<img src=\"../media/func_traj_error.png\" width=\"30%\"/>\n",
    "\n",
    "Amusingly, the dotted red and blue lines are the trajectories learnt by a classic ResNet network which is able to learn this kind of functions: the sampling of space is so discrete in Resnet that it jumps above the crossing of trajectories without even being aware of it. It's not a feature of Resnet, it's just blind imprecision ;)\n",
    "\n",
    "The same issue happens with following functions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(x) = \n",
    "\\begin{cases}\n",
    "    âˆ’1 & \\text{if ||x|| â‰¤ r1} \\\\\n",
    "    1 & \\text{if r2 â‰¤ ||x|| â‰¤ r3}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<img src=\"../media/func_circle.png\" width=\"25%\"/>\n",
    "\n",
    "Neural-ODE fails to learn such function:\n",
    "\n",
    "<img src=\"../media/func_circle_error.png\" width=\"50%\"/>\n",
    "\n",
    "In the paper, they also prove the flow of an ODE is an homeomorphism which can only continuously deform the input space. But, an homeomorphism can't create holes or tear a region apart.\n",
    "\n",
    "Thus for all those reasons, for the 2 previous kinds of functions, a Neural-ODE is stuck in one or the other point/region and can't \"jump\" across the gap.\n",
    "\n",
    "To solve this issue, an augmented version of Neural-ODE is then proposed:\n",
    "\n",
    "<img src=\"../media/func_aug_solution.png\" width=\"25%\"/>\n",
    "\n",
    "This simple augmentation by adding a few more synthetic dimensions to the vector with an initialization to 0 allows the ODE flow to solve previous cases by \"jumping\" across gaps and progressively learn to separate the regions:\n",
    "\n",
    "<img src=\"../media/func_aug_solving.png\" width=\"75%\"/>\n",
    "\n",
    "So, we know that classic Neural-ODE aren't able to represent all functions.\n",
    "\n",
    "### Is our function/task of translation relying on simplified Transformer representable with Neural-ODE?\n",
    "\n",
    "At first sight, translation task doesn't look like a continuous function. It is about language which doesn't sound like a mathematical continuous function. It maps discrete sentences to discrete sentences which are constituted of discrete tokens. Can the feature space of this function be continuous without holes?\n",
    "\n",
    "Moreover, let's consider the transformer itself. Firstly, Transformer encoder and decoder embeds input and output tokens with a positional embedding that aims at introducing the notion of position of tokens with respect to each other. This embedding is based on a sinusoidal function or a learned function. What is the impact of this positional embedding on our feature space? Does it make it continuous (or the reverse)? Secondly, self-attention and feed-forward function are all based on simple linear operations, softmax function, normalization, but also on non-linear RELU. A priori, there is no reason those functions introduce strange holes in feature space but a deeper study would be necessary to check that.\n",
    "\n",
    "Anyway, without studying further, it's hard to characterize clearly the translation task based on transformer and be sure it can be learned by a classic Neural-ODE.\n",
    "\n",
    "**Thus, it was decided to enhance NODE-Transformer with Augmented Neural ODE and test it to see if it helps improving performances.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented NODE-Transformer\n",
    "\n",
    "Please note that when augmenting the NODE-Transformer using technique above, for alignment reasons, we need to take into account the Transformer attention blocks which are multi-head blocks. So, when we augment the NODE-Transformer by `n` dimensions, it's in fact an augmentation of `n * number_attention_heads`.\n",
    "\n",
    "### Failed Experiment: Training an Augmented NODE-Transformer\n",
    "\n",
    "Augmented Neural ODE is implemented in this [github repository](https://github.com/EmilienDupont/augmented-neural-odes) based on TorchDiffEq library so it was quite easy to port it.\n",
    "\n",
    "Here are the plots of a training session of augmented full NODE-Transformer with ODE error tolerance of 0.001:\n",
    "\n",
    "\n",
    "| Best Validation Loss | Training/Validation Loss |\n",
    "|:---:|:---:|\n",
    "| <img src=\"../media/node_transformer_full_aug1_tol001_best_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_full_aug1_tol001_loss.svg\" width=\"150%\"/> | \n",
    "\n",
    "| Decoder ODE Calls | Encoder ODE Calls |\n",
    "|:---:|:---:|\n",
    "| <img src=\"../media/node_transformer_full_aug1_tol001_nfe_decoder.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_full_aug1_tol001_nfe_encoder.svg\" width=\"150%\"/> |\n",
    "\n",
    "After 7h, the training was early-stopped. The Decoder ODE calls stayed stable for one epoch, then decreasing for a couple of epochs and then increased a lot in a noisy way. Encoder ODE calls stay stable as usual. It would have taken tens of hours to converge to a minimum loss and it was already more than 30Â°C in the place due to external and GPU heat.\n",
    "\n",
    "With this early-stopping, we can't say whether Augmented NODE-Transformer allows to reach better performance than without augmentation.\n",
    "\n",
    "So if we wanted to test further, we needed to reduce required computing resources.\n",
    "\n",
    "----\n",
    "\n",
    "**Conclusion 5: No bigger GPU, no augmented full NODE-Transformer**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing NODE-Transformer computing burden\n",
    "\n",
    "We've seen in all previous experiments that NODE-Encoder seems to learn without increasing its complexity, suggesting that using the Neural-ODE on the encoding part might not be very useful compared to a classic neural network.\n",
    "\n",
    "**A very simple idea to reduce computing resources and duration is to get rid of Node-Encoder and just keep Node-Decoder**\n",
    "\n",
    "\n",
    "### Optimization 1: NODE-Transformer with NODE-Decoder only\n",
    "\n",
    "Here is the architecture of NODE-Transformer-Decoder-Only:\n",
    "\n",
    "<img src=\"../media/Node-Transformer-1.png\" width=\"50%\"/>\n",
    "\n",
    "The encoder part is the original Transformer Encoder with N layers and residual connections and the decoder part is our NODE-Decoder with 1 layer.\n",
    "\n",
    "\n",
    "#### Yet another non concluding experiment: NODE-Decoder only with augmented dimensions\n",
    "\n",
    "Here is a training session of that architecture with 1 augmented dimension and a high ODE Error Tolerance of `0.01` to reduce training duration:\n",
    "\n",
    "| Best Validation Loss | Training/Validation Loss | Decoder ODE Calls |\n",
    "|:---:|:---:|:---:|\n",
    "| <img src=\"../media/node_transformer_decoder_only_aug_1_best_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_nfe_decoder.svg\" width=\"150%\"/> |\n",
    "\n",
    "After 4h of training, the training ended stuck in an ODE solver never-ending computation. Apparently, it couldn't reach an estimated value in the error tolerance and started refining sampling steps finer and finer without finding any solution. Without any certitude but lots of imagination, we can imagine that the augmented dimension allows the ODE-Solver to transform the space. But maybe it deforms the space a bit too \"hard\" and it ends in a very perturbated or noisy landscape in which this high error tolerance of `0.01` never allows to reach a good estimation. But an in-depth study on the ODE feature space would be required to have any serious analysis...\n",
    "\n",
    "----\n",
    "\n",
    "**Conclusion 6: Neural-ODE can end in a never-ending ODE solving state**\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "> Please note that in the ODE solver used here, no limit is set to the max number of calls so it can end in this forever-computing case. If you limit the number of ODE calls, how should it be managed in the training?... To be studied!\n",
    "\n",
    "Anyway, when it fails, never abandon, push further! So reducing computation was pushed further!\n",
    "\n",
    "In Decoding path of Transformer, there are 2 parts:\n",
    "\n",
    "- A multi-head self-attention block on the output sequence,\n",
    "- A multi-head attention between this self-attention computed on the output sequence and the output of the Transformer encoder self-attention applied to the input sequence.\n",
    "\n",
    "So, we can suggest the intuition that the self-attention block on the output sequence might behave like the self-attention block applied to input sequence: it learns \"raw\" knowledge about the sequence and we showed experimentally that it doesn't take much advantage of Neural-ODE capabilities of increasing its own complexity.\n",
    "\n",
    "Despite being just an intuition, it was interesting to test this approach with the so-called _Separated Node-Decoder architecture_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization 2: NODE-Transformer with Separated NODE-Decoder\n",
    "\n",
    "<img src=\"../media/Node-Transformer-2.png\" width=\"50%\"/>\n",
    "\n",
    "The Neural-ODE is applied only on the attention between the encoder output computed on the input sequence and the self-attention on the output sequence.\n",
    "\n",
    "#### Finally a concluding experiment: Augmented NODE-Transformer with Separated NODE-Decoder\n",
    "\n",
    "Here is a training of this architecture with 1 augmented dimension, same high error tolerance of 0.01.\n",
    "\n",
    "| Best Validation Loss | Training/Validation Loss | Decoder ODE Calls |\n",
    "|:---:|:---:|:---:|\n",
    "| <img src=\"../media/node_transformer_decoder_only_aug_1_sep_best_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_sep_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_sep_nfe_decoder.svg\" width=\"150%\"/> |\n",
    "\n",
    "This training converged in around 4h.\n",
    "\n",
    "Loss reached a lower value of ~4.38 which is lower than the 4.5 obtained with 35h first training on full NODE-Transformer with lower error tolerance of 0.001. It is also much lower than the `6.5` obtained with higher error tolerance of 0.01 on full NODE-Transformer.\n",
    "\n",
    "Yet, it's still much higher than the ~3.2 loss reached by classic transformer with 1 layer in encoder/decoder. \n",
    "\n",
    "Moreover, we can remark that ODE calls started much lower than previous trainings (65 compared to the 180-200 in other trainings) and started to rise progressively with longer stability steps before rising again.\n",
    "\n",
    "Encouraging but not fully satisfying. As expected, the presence of classic Transformer self-attention limitates the computation resources required and seems to provide more stable learning. The NODE-decoder is able to use that knowledge starting from a lower level of complexity and then build more and more complex decoding knowledge. But it is not able to reach the same level of performance as the classic Transformer 1-layer decoder with residual connection pipeline. The too high error tolerance naturally might prevent from refining knowledge. It would be worth testing lower error tolerance but, as explained earlier, more computing power would be welcome again.\n",
    "\n",
    "As we obtain almost the same results as with full NODE-Transfomer, we can infer that the learning bottleneck of performance of original NODE-Transformer might happen in this part of the Decoder (ie the attention between encoder output on input sequence and self-attention on output sequence). For now, not having the computing power to test the same code with lower error tolerances, it's hard to make further suppositions on the causes.\n",
    "\n",
    "**Conclusion 7: restricting NODE-Transformer to separated NODE-Decoder reduces drastically computations as expected without impacting final performances suggesting the NODE-Transformer learning  bottleneck might be located in this part of the decoder.**\n",
    "\n",
    "**Conclusion 8: Augmented dimension didn't help improving performance importantly so our translation function based on transformer might not be among the non-representable functions by Neural-ODE.**\n",
    "\n",
    "Then it was decided to test a last solution to improve not the computation but the performance: make NODE-Transformer decoder aware of time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 3: NODE-Transformer trained with Time Dependency\n",
    "\n",
    ">This idea of time-dependency was stolen from the augmented neural-ODE code base.\n",
    "\n",
    "DOPRI5 ODE Solver is an explicit adaptive method that samples time-steps in the interval `[0.0, 1.0]` according to its algorithm and max error tolerance. The sampled time-step is provided to the embedded function, i.e our simplified Transformer neural network. But our Transformer is not aware of time, it doesn't use this time-step in its computation. So if the ODE Solver calls our network with same value at time-step `t1=0.1` and time-step `t2=0.9`, it will return the same result. Intuitively, one would like the Transformer to compute something different at different time-steps as it is not the same \"place\" in the feature space, right?\n",
    "\n",
    "So after augmenting the Neural-ODE equation with a synthetic dimension, the final vector passed to the transformer function was also augmented with the time information:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x, t) &= f(\\begin{bmatrix}\n",
    "           x \\\\\n",
    "           t \\\\\n",
    "         \\end{bmatrix})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "#### A semi-concluding experiment: NODE-Transformer with Augmented Separated NODE-Decoder and Time-Dependency\n",
    "\n",
    "Here is a training of NODE-Transformer restricted to separated NODE-Decoder augmented with one dimension and time-dependency and error tolerance of 0.01.\n",
    "\n",
    "_In next plots, dark blue and red plots are the current training. The light blue are from previous training \"Augmented NODE-Transformer with Separated NODE-Decoder\" to compare it directly._\n",
    "\n",
    "| Best Validation Loss | Training/Validation Loss | Decoder ODE Calls (dark blue) |\n",
    "|:---:|:---:|:---:|\n",
    "| <img src=\"../media/node_transformer_decoder_only_aug_1_timedep_best_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_timedep_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_sep_with_timedep_nfe_decoder.svg\" width=\"150%\"/> |\n",
    "\n",
    "We see here that the training is almost the same as without time-dependence. It reaches the same lower value of loss with almost the same curve. The only difference is in the NODE-Decoder ODE calls which start higher and rise faster with time-dependency. It sounds logical as adding time information naturally increases apparent complexity at beginning of training. Yet at the end of training, with or without time-dependency, the number of ODE calls is about the same. Without having done any further study, it might mean that it reaches the same level of complexity in both cases or maybe even the same time-steps distribution.\n",
    "\n",
    "**Conclusion 9: Adding time-dependency didn't improve our training performance but it remains an interesting idea for other Neural-ODE use-cases in which it could be more meaningful.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 4: NODE-Transformer trained with Weight Decay\n",
    "\n",
    "A last aspect was experimented: is it possible to reduce the increase of ODE calls by acting on our training process?\n",
    "\n",
    "In Paper [FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models](http://arxiv.org/abs/1810.01367), the authors remarked that number of function evaluations in Neural-ODE is reduced by using different forms of regularization such as weight decay and spectral normalization at the cost of hurting a bit performance.\n",
    "\n",
    "In our case, performance isn't really good but we decided to try weight decay as it is very easy to put in place. Weight decay is a very simple idea: after updating weights, they are multiplied by a factor below 1 to prevent those weights from growing too fast.\n",
    "\n",
    "#### Deceptive experiment: NODE-Transformer with Augmented Separated NODE-Decoder and Weight Decay\n",
    "\n",
    "We trained a NODE-Transformer restricted to separated NODE-Decoder augmented with 1 dimension and weight decay factor of 0.9 and error tolerance 0.01.\n",
    "\n",
    "_In next plots, green and grey plots are the current training. The other plots (light blue and red) are from previous training \"Augmented NODE-Transformer with Separated NODE-Decoder\" to compare it directly._\n",
    "\n",
    "| Best Validation Loss (grey) | Training/Validation Loss (green & grey) | Decoder ODE Calls (green) |\n",
    "|:---:|:---:|:---:|\n",
    "| <img src=\"../media/node_transformer_decoder_only_aug_1_sep_weight_decay_best_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_sep_weight_decay_loss.svg\" width=\"150%\"/> | <img src=\"../media/node_transformer_decoder_only_aug_1_sep_weight_decay_nfe_decoder.svg\" width=\"150%\"/> |\n",
    "\n",
    "We can see that the number of ODE calls is the same in first epoch and then decreases with weight decay and stays lower than training without weight decay for a couple of epochs. Then it increases suddenly, higher than training without weight decay at the same epoch. But then it stays quite stable despite being quite noisy. Meanwhile in the training without weight decay, the number of calls grows constantly and goes over the one without weight decay.\n",
    "\n",
    "So, it seems to lower the ODE-Solver computations globally but the impact on loss is not minor as said in [FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models](http://arxiv.org/abs/1810.01367).\n",
    "\n",
    "**Conclusion 10: using weight decay in NODE-Transformer training reduces globally the number of ODE evaluation but hurts the performance. Yet the idea that regularizations of learnable parameters can help tuning the resource requirements of an ODE Solver at the cost of some performance is really interesting.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "### Hacking TorchDiffEq Neural-ODE\n",
    "\n",
    "In this project, Pytorch was the framework used and Neural-ODE implementation was found in [torchdiffeq github](https://github.com/rtqichen/torchdiffeq).\n",
    "\n",
    "TorchDiffEq Neural-ODE code is good for basic neural networks with one input and one output. But Transformer encoder/decoder is not really a basic neural network as attention network requires multiple inputs (Q/K/V) and different options.\n",
    "\n",
    "Without going in details, we needed to extend TorchDiffEq code to manage multiple and optional parameters in `odeint_adjoint` and sub-functions. The code can be found [odeint_ext](https://github.com/mandubian/pytorch-neural-ode/tree/master/odeint_ext) and we'll see later if it's generic enough to be contribute it back to torchdiffeq project.\n",
    "\n",
    "\n",
    "### Creating NODE-Transformer with fairseq\n",
    "\n",
    "NODE-Transformer is just a new kind of Transformer as implemented in [FairSeq library](https://github.com/pytorch/fairseq).\n",
    "\n",
    "So it was just implemented as a new kind of Transformer using FairSeq API, the [NODE-Transformer](https://github.com/mandubian/pytorch-neural-ode/blob/master/node-transformer-fair/node_transformer/node_transformer.py). Implementing it wasn't so complicated, the API is quite complete, you need to read some code to be sure about what to do but nothing crazy. _The code is still raw, not yet cleaned-up and polished so don't be surprised to find weird comments or remaining useless lines in a few places._\n",
    "\n",
    "A custom [NODE-Trainer](https://github.com/mandubian/pytorch-neural-ode/blob/master/node-transformer-fair/node_transformer/node_trainer.py) was also required to integrate ODE function calls in reports. Maybe this part should be enhanced to make it more simply extensible\n",
    "\n",
    "Here are the new options to manipulate the new kind of FairSeq NODE-Transformer:\n",
    "\n",
    "```\n",
    "    --arch node_transformer    \n",
    "    --node-encoder\n",
    "    --node-decoder\n",
    "    --node-rtol 0.01\n",
    "    --node-atol 0.01\n",
    "    --node-ts [0.0, 1.0]\n",
    "    --node-augment-dims 1\n",
    "    --node-time-dependent\n",
    "    --node-separated-decoder\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "NODE-Transformer is cool juste because it's a nice kind of Transformer! But, as implemented right now, it is clearly not an efficient network, both in terms of performance and computing burden. Yet, despite the huge amount of electricity consumed for useless trainings, it was a nice trip because a lot about many topics was learnt and this trip might not be ended yet.\n",
    "\n",
    "Many other aspects would be worth studing such as:\n",
    "\n",
    "- having more computing GPU power to train NODE-Transformer with smaller error tolerances to see if performance improves at least to the level of a 1-layer classic Transformer\n",
    "- studying what \"increasing knowledge complexity\" truly means along the training process: we could explore attention matrices, gradient fields, ODE-Solver feature landscape...\n",
    "- testing Neural-ODE with attention mechanism alone on a simpler task to see how both behave together\n",
    "- studying if it's a viable idea to skip batches that take too many ODE calls and recompute them later when global network has reached a sufficient complexity level for those batches.\n",
    "- ...\n",
    "\n",
    "---- \n",
    "\n",
    "**REQUEST FOR RESOURCES: If you like this topic and have GPU resources that you can share for free and want to help perform more studies on that idea, don't hesitate to contact me on Twitter @mandubian or Github, I'd be happy to consume your resources ;)**\n",
    "\n",
    "----\n",
    "\n",
    "Thanks and have Ordinary Differential Enjoyment!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Neural Ordinary Differential Equations**, Chen & al (2018), http://arxiv.org/abs/1806.07366\n",
    "\n",
    "1. **Augmented Neural ODEs**, Dupont, Doucet, Teh (2018), http://arxiv.org/abs/1904.01681, \n",
    "\n",
    "1. **Neural ODEs as the Deep Limit of ResNets with constant weights**, Avelin, NystrÃ¶m (2019), https://arxiv.org/abs/1906.12183v1\n",
    "\n",
    "1. **FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models**, Grathwohl & al (2018), http://arxiv.org/abs/1810.01367"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
