{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./fairseq\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, progress_bar, tasks, utils\n",
    "from fairseq.data import iterators\n",
    "from fairseq.trainer import Trainer\n",
    "from fairseq.meters import AverageMeter, StopwatchMeter\n",
    "\n",
    "from node_transformer import node_transformer, node_trainer\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='node_transformer', attention_dropout=0.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='cross_entropy', curriculum=0, data='/home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=2, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=1, encoder_learned_pos=False, encoder_normalize_before=False, find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, global_sync_iter=10, keep_interval_updates=10, keep_last_epochs=10, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format='tqdm', log_interval=1000, lr=[0.0001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=24000, max_update=0, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, node_atol=0.01, node_augment_dims=1, node_decoder=True, node_encoder=False, node_max_num_steps=1000, node_method='dopri5-ext', node_rtol=0.01, node_separated_decoder=True, node_time_dependent=True, node_ts=[0.0, 1.0], num_workers=8, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_best.pt', save_dir='checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='de', target_lang='en', task='translation', tbmf_wrapper=False, tensorboard_logdir='./runs/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030', threshold_loss_scale=None, tie_adaptive_weights=False, train_subset='train', update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "#EXP_ID=\"transformer_20190703_1800\"\n",
    "#| Generate test with beam=5: BLEU4 = 26.76, 62.4/34.6/21.0/13.1 (BP=0.965, ratio=0.966, syslen=126657, reflen=131161)\n",
    "\n",
    "\n",
    "#EXP_ID=\"node_transformer_20190703_2230\"\n",
    "#| Generate test with beam=5: BLEU4 = 22.77, 57.3/29.4/16.6/9.7 (BP=1.000, ratio=1.009, syslen=132351, reflen=131161)\n",
    "\n",
    "\n",
    "#EXP_ID=\"transformer_encoder_1layer_20190704_1000\"\n",
    "#| Generate test with beam=5: BLEU4 = 24.76, 60.5/32.2/18.7/11.3 (BP=0.978, ratio=0.978, syslen=128254, reflen=131161)\n",
    "\n",
    "\n",
    "# EXP_ID=\"transformer_decoder_1layer_20190710_1530\"\n",
    "# lr = 0.01 epoch 0-27 (nfe infinite at epoch 27) ??? (not sure anymore :D)\n",
    "# lr = 0.001 epoch 28-...\n",
    "\n",
    "\n",
    "#EXP_ID=\"node_transformer_aug_time_dep_20190710_2300\"\n",
    "# encoder 1 layer transformer\n",
    "# lr = 0.001\n",
    "# rtol/atol = 0.01\n",
    "# SUFFIX = \"_1\"\n",
    "# SUB_EXP_ID=\"node_transformer_aug_time_dep_20190710_2300_1\"\n",
    "# lr = 0.001\n",
    "# adam\n",
    "# rtol/atol = 0.005\n",
    "\n",
    "\n",
    "#EXP_ID=\"transformer_encoder_decoder_1layer_20190716_0900\"\n",
    "#SUFFIX=\"\"\n",
    "# encoder/decoder 1 layer transformer\n",
    "# lr = 0.001\n",
    "\n",
    "#EXP_ID=\"node_transformer_separated_decoder_20190716_2100\"\n",
    "#SUFFIX=\"_1\"\n",
    "# separated node_decoder 1 layer - encoder 1 layer\n",
    "# rtol/atol = 0.1 then 0.01\n",
    "# lr = 0.0001\n",
    "\n",
    "#EXP_ID=\"node_transformer_separated_decoder_aug_4_20190718_0000\"\n",
    "# rtol/atol = 0.0005\n",
    "# lr = 0.001 then 0.0005\n",
    "#SUFFIX=\"_1\"\n",
    "\n",
    "EXP_ID=\"node_transformer_separated_decoder_aug_1_2_layers_20190719_1030\"\n",
    "SUFFIX=\"\"\n",
    "args = [\n",
    "    #\"/home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/wmt14_en_fr\",\n",
    "    \"/home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en\",\n",
    "    #\"--arch\", \"node_transformer_wmt_en_fr\",\n",
    "    \"--arch\", \"node_transformer\",\n",
    "    #\"--arch\", \"transformer_iwslt_de_en\",\n",
    "    \"--task\", \"translation\",\n",
    "    \"--source-lang\", \"de\",\n",
    "    \"--target-lang\", \"en\",\n",
    "    # NODE PARAMS BEGIN\n",
    "    #\"--node-encoder\",\n",
    "    \"--node-decoder\",\n",
    "    \"--node-rtol\", \"0.01\",\n",
    "    \"--node-atol\", \"0.01\",\n",
    "    #\"--node-rtol\", \"0.0005\",\n",
    "    #\"--node-atol\", \"0.0005\",\n",
    "    \"--node-ts\", \"[0.0, 1.0]\",\n",
    "    \"--node-augment-dims\", \"1\",\n",
    "    \"--node-time-dependent\",\n",
    "    \"--node-separated-decoder\",\n",
    "    # NODE PARAMS END    \n",
    "    \"--log-format\", \"tqdm\",\n",
    "    \"--max-tokens\", \"24000\",\n",
    "    #\"--max-sentences\", \"1000\",\n",
    "    \"--num-workers\", \"8\",\n",
    "    #\"--dataset-impl\", \"lazy\",\n",
    "    \"--criterion\", \"cross_entropy\",\n",
    "    #\"--label-smoothing\", \"0.1\",\n",
    "    \"--lr\", \"0.0001\",\n",
    "    #\"--lr\", \"0.0001\",\n",
    "    \"--optimizer\", \"adam\",\n",
    "    \"--save-dir\", f\"checkpoints/{EXP_ID}{SUFFIX}\",\n",
    "    \"--tensorboard-logdir\", f\"./runs/{EXP_ID}{SUFFIX}\",\n",
    "    \"--keep-interval-updates\", \"10\",\n",
    "    \"--restore-file\", \"checkpoint_best.pt\",\n",
    "    \"--keep-last-epochs\", \"10\",\n",
    "    \"--encoder-layers\", \"1\",\n",
    "    \"--decoder-layers\", \"2\",\n",
    "]\n",
    "parser = options.get_training_parser()\n",
    "#add_node_args(parser)\n",
    "args = options.parse_args_and_arch(parser, input_args=args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA device 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and not args.cpu:\n",
    "    print(\"using CUDA device\", args.device_id)\n",
    "    torch.cuda.set_device(args.device_id)\n",
    "_ = torch.manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [de] dictionary: 8848 types\n",
      "| [en] dictionary: 6632 types\n"
     ]
    }
   ],
   "source": [
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "task = tasks.setup_task(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| /home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\n"
     ]
    }
   ],
   "source": [
    "# Load valid dataset (we load training data below, based on the latest checkpoint)\n",
    "for valid_sub_split in args.valid_subset.split(','):\n",
    "    task.load_dataset(valid_sub_split, combine=True, epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeTransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): Embedding(8848, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): NodeTransformerDecoder(\n",
      "    (embed_tokens): Embedding(6632, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): NodeTransformerDecoderLayer_Separated(\n",
      "        (func_self_attn): TransformerDecoderLayerFunc_SelfAttn(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (func_ode): NodeTransformerDecoderLayerFunc_EncoderAttn(\n",
      "          (encoder_attn): MultiheadAttention(\n",
      "            (out_proj): Linear(in_features=528, out_features=528, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm(torch.Size([528]), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=528, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=520, bias=True)\n",
      "          (final_layer_norm): LayerNorm(torch.Size([520]), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=520, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): NodeTransformerDecoderLayer_Separated(\n",
      "        (func_self_attn): TransformerDecoderLayerFunc_SelfAttn(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (func_ode): NodeTransformerDecoderLayerFunc_EncoderAttn(\n",
      "          (encoder_attn): MultiheadAttention(\n",
      "            (out_proj): Linear(in_features=528, out_features=528, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm(torch.Size([528]), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=528, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=520, bias=True)\n",
      "          (final_layer_norm): LayerNorm(torch.Size([520]), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=520, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| model node_transformer, criterion CrossEntropyCriterion\n",
      "| num. model params: 23649008 (num. trained: 23649008)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build model and criterion\n",
    "model = task.build_model(args)\n",
    "criterion = task.build_criterion(args)\n",
    "print(model)\n",
    "print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n",
    "print('| num. model params: {} (num. trained: {})'.format(\n",
    "    sum(p.numel() for p in model.parameters()),\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| training on 1 GPUs\n",
      "| max tokens per GPU = 24000 and max sentences per GPU = None\n"
     ]
    }
   ],
   "source": [
    "# Build trainer\n",
    "trainer = node_trainer.Trainer(args, task, model, criterion)\n",
    "print('| training on {} GPUs'.format(args.distributed_world_size))\n",
    "print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n",
    "    args.max_tokens,\n",
    "    args.max_sentences,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| no existing checkpoint found checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030/checkpoint_best.pt\n",
      "| loading train data for epoch 0\n",
      "| /home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\n"
     ]
    }
   ],
   "source": [
    "## Load the latest checkpoint if one is available and restore the\n",
    "# corresponding train iterator\n",
    "#save_dir_keep = args.save_dir\n",
    "#args.save_dir = f\"checkpoints/{EXP_ID}{SUFFIX}\"\n",
    "#args.reset_optimizer = True\n",
    "extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n",
    "#args.save_dir = save_dir_keep\n",
    "#args.reset_optimizer = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 001 | loss 8.848 | ppl 460.69 | wps 2093 | ups 0 | wpb 19263.971 | bsz 781.654 | num_updates 205 | lr 0.0001 | gnorm 1.556 | clip 0.000 | oom 0.000 | wall 1890 | train_wall 1886 | nfe_decoder 150.107\n",
      "| epoch 001 | valid on 'valid' subset | loss 7.842 | ppl 229.42 | num_updates 205\n",
      "| saved checkpoint checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030/checkpoint1.pt (epoch 1 @ 205 updates) (writing took 0.3585937023162842 seconds)\n",
      "| epoch 002 | loss 7.291 | ppl 156.61 | wps 1955 | ups 0 | wpb 19263.971 | bsz 781.654 | num_updates 410 | lr 0.0001 | gnorm 1.546 | clip 0.000 | oom 0.000 | wall 3922 | train_wall 3906 | nfe_decoder 154.556\n",
      "| epoch 002 | valid on 'valid' subset | loss 6.870 | ppl 116.96 | num_updates 410 | best_loss 6.86984\n",
      "| saved checkpoint checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030/checkpoint2.pt (epoch 2 @ 410 updates) (writing took 0.5826730728149414 seconds)\n",
      "| epoch 003 | loss 6.590 | ppl 96.33 | wps 1909 | ups 0 | wpb 19263.971 | bsz 781.654 | num_updates 615 | lr 0.0001 | gnorm 1.359 | clip 0.000 | oom 0.000 | wall 6005 | train_wall 5977 | nfe_decoder 156.927\n",
      "| epoch 003 | valid on 'valid' subset | loss 6.323 | ppl 80.05 | num_updates 615 | best_loss 6.32276\n",
      "| saved checkpoint checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030/checkpoint3.pt (epoch 3 @ 615 updates) (writing took 0.5776889324188232 seconds)\n",
      "| epoch 004 | loss 6.156 | ppl 71.31 | wps 1830 | ups 0 | wpb 19263.971 | bsz 781.654 | num_updates 820 | lr 0.0001 | gnorm 1.273 | clip 0.000 | oom 0.000 | wall 8174 | train_wall 8133 | nfe_decoder 159.283\n",
      "| epoch 004 | valid on 'valid' subset | loss 6.018 | ppl 64.79 | num_updates 820 | best_loss 6.01764\n",
      "| saved checkpoint checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030/checkpoint4.pt (epoch 4 @ 820 updates) (writing took 0.5855917930603027 seconds)\n",
      "| epoch 005 | loss 5.862 | ppl 58.15 | wps 1788 | ups 0 | wpb 19263.971 | bsz 781.654 | num_updates 1025 | lr 0.0001 | gnorm 1.252 | clip 0.000 | oom 0.000 | wall 10393 | train_wall 10339 | nfe_decoder 161.229\n",
      "| epoch 005 | valid on 'valid' subset | loss 5.747 | ppl 53.69 | num_updates 1025 | best_loss 5.74662\n",
      "| saved checkpoint checkpoints/node_transformer_separated_decoder_aug_1_2_layers_20190719_1030/checkpoint5.pt (epoch 5 @ 1025 updates) (writing took 0.5840761661529541 seconds)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5074426f7b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_lr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_epoch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_itr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5074426f7b58>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, trainer, task, epoch_itr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#timesteps = torch.FloatTensor(args.ts).to()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations_in_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mlog_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/pytorch-neural-ode/node-transformer-fair/node_transformer/node_trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, samples, dummy_batch, raise_oom)\u001b[0m\n\u001b[1;32m    266\u001b[0m                     loss, sample_size, logging_output = self.task.train_step(\n\u001b[1;32m    267\u001b[0m                         \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                         \u001b[0mignore_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     )\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, sample, model, criterion, optimizer, ignore_grad)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/fairseq/optim/fairseq_optimizer.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmultiply_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import validate\n",
    "\n",
    "def train(args, trainer, task, epoch_itr):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    # Update parameters every N batches\n",
    "    update_freq = args.update_freq[epoch_itr.epoch - 1] \\\n",
    "        if epoch_itr.epoch <= len(args.update_freq) else args.update_freq[-1]\n",
    "\n",
    "    # Initialize data iterator\n",
    "    itr = epoch_itr.next_epoch_itr(\n",
    "        fix_batches_to_gpus=args.fix_batches_to_gpus,\n",
    "        shuffle=(epoch_itr.epoch >= args.curriculum),\n",
    "    )\n",
    "    itr = iterators.GroupedIterator(itr, update_freq)\n",
    "    progress = progress_bar.build_progress_bar(\n",
    "        args, itr, epoch_itr.epoch, no_progress_bar='simple',\n",
    "    )\n",
    "\n",
    "    extra_meters = collections.defaultdict(lambda: AverageMeter())\n",
    "    valid_subsets = args.valid_subset.split(',')\n",
    "    max_update = args.max_update or math.inf\n",
    "    \n",
    "    #timesteps = torch.FloatTensor(args.ts).to()\n",
    "    for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch):\n",
    "        log_output = trainer.train_step(samples)\n",
    "        if log_output is None:\n",
    "            continue\n",
    "\n",
    "        # log mid-epoch stats\n",
    "        stats = get_training_stats(trainer)\n",
    "        for k, v in log_output.items():\n",
    "            if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']:\n",
    "                continue  # these are already logged above\n",
    "            if 'loss' in k:\n",
    "                extra_meters[k].update(v, log_output['sample_size'])\n",
    "            else:\n",
    "                extra_meters[k].update(v)\n",
    "            stats[k] = extra_meters[k].avg\n",
    "        progress.log(stats, tag='train', step=stats['num_updates'])\n",
    "\n",
    "        # ignore the first mini-batch in words-per-second calculation\n",
    "        if i == 0:\n",
    "            trainer.get_meter('wps').reset()\n",
    "\n",
    "        num_updates = trainer.get_num_updates()\n",
    "        if (\n",
    "            not args.disable_validation\n",
    "            and args.save_interval_updates > 0\n",
    "            and num_updates % args.save_interval_updates == 0\n",
    "            and num_updates > 0\n",
    "        ):\n",
    "            valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n",
    "            checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
    "\n",
    "        if num_updates >= max_update:\n",
    "            break\n",
    "\n",
    "    # log end-of-epoch stats\n",
    "    stats = get_training_stats(trainer)\n",
    "    for k, meter in extra_meters.items():\n",
    "        stats[k] = meter.avg\n",
    "    progress.print(stats, tag='train', step=stats['num_updates'])\n",
    "\n",
    "    # reset training meters\n",
    "    for k in [\n",
    "        'train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip',\n",
    "    ]:\n",
    "        meter = trainer.get_meter(k)\n",
    "        if meter is not None:\n",
    "            meter.reset()\n",
    "\n",
    "\n",
    "def get_training_stats(trainer):\n",
    "    stats = collections.OrderedDict()\n",
    "    stats['loss'] = trainer.get_meter('train_loss')\n",
    "    if trainer.get_meter('train_nll_loss').count > 0:\n",
    "        nll_loss = trainer.get_meter('train_nll_loss')\n",
    "        stats['nll_loss'] = nll_loss\n",
    "    else:\n",
    "        nll_loss = trainer.get_meter('train_loss')\n",
    "    stats['ppl'] = utils.get_perplexity(nll_loss.avg)\n",
    "    stats['wps'] = trainer.get_meter('wps')\n",
    "    stats['ups'] = trainer.get_meter('ups')\n",
    "    stats['wpb'] = trainer.get_meter('wpb')\n",
    "    stats['bsz'] = trainer.get_meter('bsz')\n",
    "    stats['num_updates'] = trainer.get_num_updates()\n",
    "    stats['lr'] = trainer.get_lr()\n",
    "    stats['gnorm'] = trainer.get_meter('gnorm')\n",
    "    stats['clip'] = trainer.get_meter('clip')\n",
    "    stats['oom'] = trainer.get_meter('oom')\n",
    "    if trainer.get_meter('loss_scale') is not None:\n",
    "        stats['loss_scale'] = trainer.get_meter('loss_scale')\n",
    "    stats['wall'] = round(trainer.get_meter('wall').elapsed_time)\n",
    "    stats['train_wall'] = trainer.get_meter('train_wall')\n",
    "    if trainer._model.has_node_encoder:\n",
    "        stats['nfe_encoder'] = trainer.get_meter('nfe_encoder')\n",
    "    if trainer._model.has_node_decoder:\n",
    "        stats['nfe_decoder'] = trainer.get_meter('nfe_decoder')\n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "# Train until the learning rate gets too small\n",
    "max_epoch = args.max_epoch or math.inf\n",
    "max_update = args.max_update or math.inf\n",
    "lr = trainer.get_lr()\n",
    "train_meter = StopwatchMeter()\n",
    "train_meter.start()\n",
    "valid_losses = [None]\n",
    "valid_subsets = args.valid_subset.split(',')\n",
    "while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n",
    "    # train for one epoch\n",
    "    train(args, trainer, task, epoch_itr)\n",
    "\n",
    "    if not args.disable_validation and epoch_itr.epoch % args.validate_interval == 0:\n",
    "        valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n",
    "    else:\n",
    "        valid_losses = [None]\n",
    "\n",
    "    # only use first validation loss to update the learning rate\n",
    "    lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n",
    "\n",
    "    # save checkpoint\n",
    "    if epoch_itr.epoch % args.save_interval == 0:\n",
    "        checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
    "\n",
    "    if ':' in getattr(args, 'data', ''):\n",
    "        # sharded data: get train iterator for next epoch\n",
    "        epoch_itr = trainer.get_train_iterator(epoch_itr.epoch)\n",
    "train_meter.stop()\n",
    "print('| done training in {:.1f} seconds'.format(train_meter.sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
