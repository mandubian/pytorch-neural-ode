{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./fairseq\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, progress_bar, tasks, utils\n",
    "from fairseq.data import iterators\n",
    "from fairseq.trainer import Trainer\n",
    "from fairseq.meters import AverageMeter, StopwatchMeter\n",
    "\n",
    "from node_transformer import node_transformer, node_trainer\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='relu', adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='node_transformer', attention_dropout=0.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='cross_entropy', curriculum=0, data='/home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layers=1, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=1, encoder_learned_pos=False, encoder_normalize_before=False, find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, global_sync_iter=10, keep_interval_updates=10, keep_last_epochs=10, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format='tqdm', log_interval=1000, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=8000, max_update=0, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, node_atol=0.01, node_augment_dims=0, node_decoder=False, node_encoder=False, node_max_num_steps=1000, node_method='dopri5-ext', node_rtol=0.01, node_time_dependent=False, node_ts=[0.0, 1.0], num_workers=8, optimizer='nag', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_best.pt', save_dir='checkpoints/transformer_encoder_decoder_1layer_20190716_0900', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='de', target_lang='en', task='translation', tbmf_wrapper=False, tensorboard_logdir='./runs/transformer_encoder_decoder_1layer_20190716_0900', threshold_loss_scale=None, tie_adaptive_weights=False, train_subset='train', update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "#EXP_ID=\"transformer_20190703_1800\"\n",
    "#| Generate test with beam=5: BLEU4 = 26.76, 62.4/34.6/21.0/13.1 (BP=0.965, ratio=0.966, syslen=126657, reflen=131161)\n",
    "\n",
    "\n",
    "#EXP_ID=\"node_transformer_20190703_2230\"\n",
    "#| Generate test with beam=5: BLEU4 = 22.77, 57.3/29.4/16.6/9.7 (BP=1.000, ratio=1.009, syslen=132351, reflen=131161)\n",
    "\n",
    "\n",
    "#EXP_ID=\"transformer_encoder_1layer_20190704_1000\"\n",
    "#| Generate test with beam=5: BLEU4 = 24.76, 60.5/32.2/18.7/11.3 (BP=0.978, ratio=0.978, syslen=128254, reflen=131161)\n",
    "\n",
    "\n",
    "# EXP_ID=\"transformer_decoder_1layer_20190710_1530\"\n",
    "# lr = 0.01 epoch 0-27 (nfe infinite at epoch 27) ??? (not sure anymore :D)\n",
    "# lr = 0.001 epoch 28-...\n",
    "\n",
    "\n",
    "#EXP_ID=\"node_transformer_aug_time_dep_20190710_2300\"\n",
    "# encoder 1 layer transformer\n",
    "# lr = 0.001\n",
    "# rtol/atol = 0.01\n",
    "# SUFFIX = \"_1\"\n",
    "# SUB_EXP_ID=\"node_transformer_aug_time_dep_20190710_2300_1\"\n",
    "# lr = 0.001\n",
    "# adam\n",
    "# rtol/atol = 0.005\n",
    "\n",
    "\n",
    "EXP_ID=\"transformer_encoder_decoder_1layer_20190716_0900\"\n",
    "SUFFIX=\"\"\n",
    "# encoder/decoder 1 layer transformer\n",
    "# lr = 0.001\n",
    "\n",
    "args = [\n",
    "    #\"/home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/wmt14_en_fr\",\n",
    "    \"/home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en\",\n",
    "    #\"--arch\", \"node_transformer_wmt_en_fr\",\n",
    "    \"--arch\", \"node_transformer\",\n",
    "    #\"--arch\", \"transformer_iwslt_de_en\",\n",
    "    \"--task\", \"translation\",\n",
    "    \"--source-lang\", \"de\",\n",
    "    \"--target-lang\", \"en\",\n",
    "    # NODE PARAMS BEGIN\n",
    "    #\"--node-encoder\",\n",
    "    #\"--node-decoder\",\n",
    "    #\"--node-rtol\", \"0.005\",\n",
    "    #\"--node-atol\", \"0.005\",\n",
    "    #\"--node-ts\", \"[0.0, 1.0]\",\n",
    "    #\"--node-augment-dims\", \"1\",\n",
    "    #\"--node-time-dependent\",\n",
    "    # NODE PARAMS END    \n",
    "    \"--log-format\", \"tqdm\",\n",
    "    \"--max-tokens\", \"8000\",\n",
    "    #\"--max-sentences\", \"1000\",\n",
    "    \"--num-workers\", \"8\",\n",
    "    #\"--dataset-impl\", \"lazy\",\n",
    "    \"--criterion\", \"cross_entropy\",\n",
    "    #\"--label-smoothing\", \"0.1\",\n",
    "    \"--lr\", \"0.001\",\n",
    "    #\"--lr\", \"0.0001\",\n",
    "    #\"--optimizer\", \"adam\",\n",
    "    \"--save-dir\", f\"checkpoints/{EXP_ID}{SUFFIX}\",\n",
    "    \"--tensorboard-logdir\", f\"./runs/{EXP_ID}{SUFFIX}\",\n",
    "    \"--keep-interval-updates\", \"10\",\n",
    "    \"--restore-file\", \"checkpoint_best.pt\",\n",
    "    \"--keep-last-epochs\", \"10\",\n",
    "    \"--encoder-layers\", \"1\",\n",
    "    \"--decoder-layers\", \"1\",\n",
    "]\n",
    "parser = options.get_training_parser()\n",
    "#add_node_args(parser)\n",
    "args = options.parse_args_and_arch(parser, input_args=args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA device 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and not args.cpu:\n",
    "    print(\"using CUDA device\", args.device_id)\n",
    "    torch.cuda.set_device(args.device_id)\n",
    "_ = torch.manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [de] dictionary: 8848 types\n",
      "| [en] dictionary: 6632 types\n"
     ]
    }
   ],
   "source": [
    "# Setup task, e.g., translation, language modeling, etc.\n",
    "task = tasks.setup_task(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| /home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\n"
     ]
    }
   ],
   "source": [
    "# Load valid dataset (we load training data below, based on the latest checkpoint)\n",
    "for valid_sub_split in args.valid_subset.split(','):\n",
    "    task.load_dataset(valid_sub_split, combine=True, epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeTransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): Embedding(8848, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): Embedding(6632, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| model node_transformer, criterion CrossEntropyCriterion\n",
      "| num. model params: 18677760 (num. trained: 18677760)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build model and criterion\n",
    "model = task.build_model(args)\n",
    "criterion = task.build_criterion(args)\n",
    "print(model)\n",
    "print('| model {}, criterion {}'.format(args.arch, criterion.__class__.__name__))\n",
    "print('| num. model params: {} (num. trained: {})'.format(\n",
    "    sum(p.numel() for p in model.parameters()),\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| training on 1 GPUs\n",
      "| max tokens per GPU = 8000 and max sentences per GPU = None\n"
     ]
    }
   ],
   "source": [
    "# Build trainer\n",
    "trainer = node_trainer.Trainer(args, task, model, criterion)\n",
    "print('| training on {} GPUs'.format(args.distributed_world_size))\n",
    "print('| max tokens per GPU = {} and max sentences per GPU = {}'.format(\n",
    "    args.max_tokens,\n",
    "    args.max_sentences,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| no existing checkpoint found checkpoints/transformer_encoder_decoder_1layer_20190716_0900/checkpoint_best.pt\n",
      "| loading train data for epoch 0\n",
      "| /home/mandubian/notebooks/pytorch-neural-ode/node-transformer-fair/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en train de-en 160239 examples\n"
     ]
    }
   ],
   "source": [
    "## Load the latest checkpoint if one is available and restore the\n",
    "# corresponding train iterator\n",
    "save_dir_keep = args.save_dir\n",
    "args.save_dir = f\"checkpoints/{EXP_ID}\"\n",
    "args.reset_optimizer = True\n",
    "extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n",
    "args.save_dir = save_dir_keep\n",
    "args.reset_optimizer = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 045 | loss 5.643 | ppl 49.98 | wps 1556 | ups 0 | wpb 13386.827 | bsz 543.183 | num_updates 295 | lr 0.001 | gnorm 1.934 | clip 0.003 | oom 0.000 | wall 2557 | train_wall 104536 | nfe_decoder 119.920\n",
      "| epoch 045 | valid on 'valid' subset | loss 4.997 | ppl 31.94 | num_updates 295\n",
      "| saved checkpoint checkpoints/node_transformer_aug_time_dep_20190710_2300/checkpoint45.pt (epoch 45 @ 295 updates) (writing took 0.4802522659301758 seconds)\n",
      "| epoch 046 | loss 4.670 | ppl 25.45 | wps 1272 | ups 0 | wpb 13386.827 | bsz 543.183 | num_updates 590 | lr 0.001 | gnorm 1.264 | clip 0.000 | oom 0.000 | wall 5678 | train_wall 107642 | nfe_decoder 120.795\n",
      "| epoch 046 | valid on 'valid' subset | loss 4.670 | ppl 25.46 | num_updates 590 | best_loss 4.67031\n",
      "| saved checkpoint checkpoints/node_transformer_aug_time_dep_20190710_2300/checkpoint46.pt (epoch 46 @ 590 updates) (writing took 0.4254164695739746 seconds)\n",
      "| epoch 047 | loss 4.400 | ppl 21.12 | wps 1128 | ups 0 | wpb 13386.827 | bsz 543.183 | num_updates 885 | lr 0.001 | gnorm 1.138 | clip 0.000 | oom 0.000 | wall 9194 | train_wall 111141 | nfe_decoder 122.002\n",
      "| epoch 047 | valid on 'valid' subset | loss 4.660 | ppl 25.27 | num_updates 885 | best_loss 4.65952\n",
      "| saved checkpoint checkpoints/node_transformer_aug_time_dep_20190710_2300/checkpoint47.pt (epoch 47 @ 885 updates) (writing took 0.4285438060760498 seconds)\n",
      "| epoch 048 | loss 4.272 | ppl 19.32 | wps 882 | ups 0 | wpb 13386.827 | bsz 543.183 | num_updates 1180 | lr 0.001 | gnorm 0.993 | clip 0.000 | oom 0.000 | wall 13686 | train_wall 115612 | nfe_decoder 124.031\n",
      "| epoch 048 | valid on 'valid' subset | loss 4.502 | ppl 22.66 | num_updates 1180 | best_loss 4.50229\n",
      "| saved checkpoint checkpoints/node_transformer_aug_time_dep_20190710_2300/checkpoint48.pt (epoch 48 @ 1180 updates) (writing took 0.4120910167694092 seconds)\n",
      "| epoch 049 | loss 4.213 | ppl 18.55 | wps 842 | ups 0 | wpb 13386.827 | bsz 543.183 | num_updates 1475 | lr 0.001 | gnorm 1.011 | clip 0.000 | oom 0.000 | wall 18396 | train_wall 120299 | nfe_decoder 126.179\n",
      "| epoch 049 | valid on 'valid' subset | loss 4.584 | ppl 23.98 | num_updates 1475 | best_loss 4.50229\n",
      "| saved checkpoint checkpoints/node_transformer_aug_time_dep_20190710_2300/checkpoint49.pt (epoch 49 @ 1475 updates) (writing took 0.2553446292877197 seconds)\n"
     ]
    }
   ],
   "source": [
    "from train import validate\n",
    "\n",
    "def train(args, trainer, task, epoch_itr):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    # Update parameters every N batches\n",
    "    update_freq = args.update_freq[epoch_itr.epoch - 1] \\\n",
    "        if epoch_itr.epoch <= len(args.update_freq) else args.update_freq[-1]\n",
    "\n",
    "    # Initialize data iterator\n",
    "    itr = epoch_itr.next_epoch_itr(\n",
    "        fix_batches_to_gpus=args.fix_batches_to_gpus,\n",
    "        shuffle=(epoch_itr.epoch >= args.curriculum),\n",
    "    )\n",
    "    itr = iterators.GroupedIterator(itr, update_freq)\n",
    "    progress = progress_bar.build_progress_bar(\n",
    "        args, itr, epoch_itr.epoch, no_progress_bar='simple',\n",
    "    )\n",
    "\n",
    "    extra_meters = collections.defaultdict(lambda: AverageMeter())\n",
    "    valid_subsets = args.valid_subset.split(',')\n",
    "    max_update = args.max_update or math.inf\n",
    "    \n",
    "    #timesteps = torch.FloatTensor(args.ts).to()\n",
    "    for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch):\n",
    "        log_output = trainer.train_step(samples)\n",
    "        if log_output is None:\n",
    "            continue\n",
    "\n",
    "        # log mid-epoch stats\n",
    "        stats = get_training_stats(trainer)\n",
    "        for k, v in log_output.items():\n",
    "            if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']:\n",
    "                continue  # these are already logged above\n",
    "            if 'loss' in k:\n",
    "                extra_meters[k].update(v, log_output['sample_size'])\n",
    "            else:\n",
    "                extra_meters[k].update(v)\n",
    "            stats[k] = extra_meters[k].avg\n",
    "        progress.log(stats, tag='train', step=stats['num_updates'])\n",
    "\n",
    "        # ignore the first mini-batch in words-per-second calculation\n",
    "        if i == 0:\n",
    "            trainer.get_meter('wps').reset()\n",
    "\n",
    "        num_updates = trainer.get_num_updates()\n",
    "        if (\n",
    "            not args.disable_validation\n",
    "            and args.save_interval_updates > 0\n",
    "            and num_updates % args.save_interval_updates == 0\n",
    "            and num_updates > 0\n",
    "        ):\n",
    "            valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n",
    "            checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
    "\n",
    "        if num_updates >= max_update:\n",
    "            break\n",
    "\n",
    "    # log end-of-epoch stats\n",
    "    stats = get_training_stats(trainer)\n",
    "    for k, meter in extra_meters.items():\n",
    "        stats[k] = meter.avg\n",
    "    progress.print(stats, tag='train', step=stats['num_updates'])\n",
    "\n",
    "    # reset training meters\n",
    "    for k in [\n",
    "        'train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip',\n",
    "    ]:\n",
    "        meter = trainer.get_meter(k)\n",
    "        if meter is not None:\n",
    "            meter.reset()\n",
    "\n",
    "\n",
    "def get_training_stats(trainer):\n",
    "    stats = collections.OrderedDict()\n",
    "    stats['loss'] = trainer.get_meter('train_loss')\n",
    "    if trainer.get_meter('train_nll_loss').count > 0:\n",
    "        nll_loss = trainer.get_meter('train_nll_loss')\n",
    "        stats['nll_loss'] = nll_loss\n",
    "    else:\n",
    "        nll_loss = trainer.get_meter('train_loss')\n",
    "    stats['ppl'] = utils.get_perplexity(nll_loss.avg)\n",
    "    stats['wps'] = trainer.get_meter('wps')\n",
    "    stats['ups'] = trainer.get_meter('ups')\n",
    "    stats['wpb'] = trainer.get_meter('wpb')\n",
    "    stats['bsz'] = trainer.get_meter('bsz')\n",
    "    stats['num_updates'] = trainer.get_num_updates()\n",
    "    stats['lr'] = trainer.get_lr()\n",
    "    stats['gnorm'] = trainer.get_meter('gnorm')\n",
    "    stats['clip'] = trainer.get_meter('clip')\n",
    "    stats['oom'] = trainer.get_meter('oom')\n",
    "    if trainer.get_meter('loss_scale') is not None:\n",
    "        stats['loss_scale'] = trainer.get_meter('loss_scale')\n",
    "    stats['wall'] = round(trainer.get_meter('wall').elapsed_time)\n",
    "    stats['train_wall'] = trainer.get_meter('train_wall')\n",
    "    if trainer._model.has_node_encoder:\n",
    "        stats['nfe_encoder'] = trainer.get_meter('nfe_encoder')\n",
    "    if trainer._model.has_node_decoder:\n",
    "        stats['nfe_decoder'] = trainer.get_meter('nfe_decoder')\n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "# Train until the learning rate gets too small\n",
    "max_epoch = args.max_epoch or math.inf\n",
    "max_update = args.max_update or math.inf\n",
    "lr = trainer.get_lr()\n",
    "train_meter = StopwatchMeter()\n",
    "train_meter.start()\n",
    "valid_losses = [None]\n",
    "valid_subsets = args.valid_subset.split(',')\n",
    "while lr > args.min_lr and epoch_itr.epoch < max_epoch and trainer.get_num_updates() < max_update:\n",
    "    # train for one epoch\n",
    "    train(args, trainer, task, epoch_itr)\n",
    "\n",
    "    if not args.disable_validation and epoch_itr.epoch % args.validate_interval == 0:\n",
    "        valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n",
    "    else:\n",
    "        valid_losses = [None]\n",
    "\n",
    "    # only use first validation loss to update the learning rate\n",
    "    lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n",
    "\n",
    "    # save checkpoint\n",
    "    if epoch_itr.epoch % args.save_interval == 0:\n",
    "        checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n",
    "\n",
    "    if ':' in getattr(args, 'data', ''):\n",
    "        # sharded data: get train iterator for next epoch\n",
    "        epoch_itr = trainer.get_train_iterator(epoch_itr.epoch)\n",
    "train_meter.stop()\n",
    "print('| done training in {:.1f} seconds'.format(train_meter.sum))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
